{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat C\n",
    "\n",
    "## Setup\n",
    "\n",
    "* Importem les llibreries\n",
    "* Configurem pandas\n",
    "* Importem el dataset\n",
    "* Inspeccionem les dimensions de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APARTADO C\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import normaltest, kstest, boxcox\n",
    "\n",
    "\n",
    "# Visualitzarem 4 decimals per mostra\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "dataset = pd.read_csv(\"Consumo_cerveja.txt\", header=0, delimiter=',', decimal=\".\", names=[\"DATA\",\"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\",\"FINSEM\",\"CONSUM\"])\n",
    "dataset = dataset.assign(TEMP_DIFF= dataset.TEMPMAX - dataset.TEMPMIN)\n",
    "dataset = dataset.assign(TEMP_SUM= (dataset.TEMPMAX + dataset.TEMPMIN + dataset.TEMPMED))\n",
    "dataset = dataset.assign(TEMP_MUL= (dataset.TEMPMAX * dataset.TEMPMIN * dataset.TEMPMED))\n",
    "'''\n",
    "dataset = dataset.assign(MONTH = dataset.DATA.apply(lambda data: data.split(\"-\")[1]))\n",
    "dataset = dataset.assign(JANUARY = dataset.MONTH.apply(lambda month: 1 if month == '01' else 0))\n",
    "dataset = dataset.assign(FEBRUARY = dataset.MONTH.apply(lambda month: 1 if month == '02' else 0))\n",
    "dataset = dataset.assign(MARCH = dataset.MONTH.apply(lambda month: 1 if month == '03' else 0))\n",
    "dataset = dataset.assign(APRIL = dataset.MONTH.apply(lambda month: 1 if month == '04' else 0))\n",
    "dataset = dataset.assign(MAY = dataset.MONTH.apply(lambda month: 1 if month == '05' else 0))\n",
    "dataset = dataset.assign(JUNE = dataset.MONTH.apply(lambda month: 1 if month == '06' else 0))\n",
    "dataset = dataset.assign(JULY = dataset.MONTH.apply(lambda month: 1 if month == '07' else 0))\n",
    "dataset = dataset.assign(AUGUST = dataset.MONTH.apply(lambda month: 1 if month == '08' else 0))\n",
    "dataset = dataset.assign(SEPTEMBER = dataset.MONTH.apply(lambda month: 1 if month == '09' else 0))\n",
    "dataset = dataset.assign(OCTOBER = dataset.MONTH.apply(lambda month: 1 if month == '10' else 0))\n",
    "dataset = dataset.assign(NOVEMBER = dataset.MONTH.apply(lambda month: 1 if month == '11' else 0))\n",
    "dataset = dataset.assign(DECEMBER = dataset.MONTH.apply(lambda month: 1 if month == '12' else 0))\n",
    "dataset = dataset.assign(MONTH = dataset.MONTH.apply(lambda month: int(month)))\n",
    "'''\n",
    "prec_norm, _ = boxcox(dataset[\"PREC\"] + 0.01)\n",
    "dataset = dataset.assign(PREC_NORM=prec_norm)\n",
    "#dataset = dataset[[\"DATA\",\"MONTH\", \"JANUARY\", \"FEBRUARY\", \"MARCH\", \"APRIL\", \"MAY\", \"JUNE\", \"JULY\", \"AUGUST\", \"SEPTEMBER\", \"OCTOBER\", \"NOVEMBER\", \"DECEMBER\",  \"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\", \"PREC_NORM\",\"FINSEM\",\"TEMP_DIFF\", \"TEMP_SUM\", \"TEMP_MUL\", \"CONSUM\"]]\n",
    "dataset = dataset[[\"DATA\", \"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\", \"PREC_NORM\",\"FINSEM\",\"TEMP_DIFF\", \"TEMP_SUM\", \"TEMP_MUL\", \"CONSUM\"]]\n",
    "data = dataset.values\n",
    "x = data[:, 1:-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "print(\"Dimensionalitat de la BBDD:\", dataset.shape)\n",
    "print(\"Dimensionalitat de les entrades X\", x.shape)\n",
    "print(\"Dimensionalitat de l'atribut Y\", y.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrem les característiques del dataset\n",
    "\n",
    "* Número d'entrades\n",
    "* Descripció de les columnes\n",
    "    * Nom de cada columna\n",
    "    * Quantitat de registres amb valor\n",
    "    * Si pot ser null o no\n",
    "    * Tipus de dada\n",
    "\n",
    "La part del tipus de dada es especialment important. Si les dades no s'importen com a números no podrem treballar correctament amb el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busquem valors nulls\n",
    "\n",
    "No n'hi ha cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per comptar el nombre de valors no existents:\")\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualitzem els primers registres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per visualitzar les primeres 5 mostres de la BBDD:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per veure estadístiques dels atributs numèrics de la BBDD:\")\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogrames i Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(10,2, figsize=(10, 30))\n",
    "\n",
    "# \"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\",\"FINSEM\", \"CONSUM\"\n",
    "sns.histplot(x=dataset[\"TEMPMED\"], color='yellow', ax=axes[0,0], bins=30)\n",
    "sns.histplot(x=dataset[\"TEMPMIN\"], color='blue', ax=axes[1, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"TEMPMAX\"], color='green', ax=axes[2, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"PREC\"], color='red', ax=axes[3, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"PREC_NORM\"], color='purple', ax=axes[4, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"FINSEM\"], color='orange', ax=axes[5, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"TEMP_DIFF\"], color='pink', ax=axes[6, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"TEMP_SUM\"], color='skyblue', ax=axes[7, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"TEMP_MUL\"], color='lightgreen', ax=axes[8, 0], bins=30)\n",
    "sns.histplot(x=dataset[\"CONSUM\"], color='yellow', ax=axes[9, 0], bins=30)\n",
    "\n",
    "\n",
    "sns.boxplot(data=dataset[\"TEMPMED\"], color='yellow', ax=axes[0,1])\n",
    "sns.boxplot(data=dataset[\"TEMPMIN\"], color='blue', ax=axes[1, 1])\n",
    "sns.boxplot(data=dataset[\"TEMPMAX\"], color='green', ax=axes[2, 1])\n",
    "sns.boxplot(data=dataset[\"PREC\"], color='red', ax=axes[3, 1])\n",
    "sns.boxplot(data=dataset[\"PREC_NORM\"], color='purple', ax=axes[4, 1])\n",
    "sns.boxplot(data=dataset[\"FINSEM\"], color='orange', ax=axes[5, 1])\n",
    "sns.boxplot(data=dataset[\"TEMP_DIFF\"], color='pink', ax=axes[6, 1])\n",
    "sns.boxplot(data=dataset[\"TEMP_SUM\"], color='skyblue', ax=axes[7, 1])\n",
    "sns.boxplot(data=dataset[\"TEMP_MUL\"], color='lightgreen', ax=axes[8, 1])\n",
    "sns.boxplot(data=dataset[\"CONSUM\"], color='yellow', ax=axes[9, 1])\n",
    "\n",
    "\n",
    "plt.subplots_adjust(bottom=0, right=0.7, top=1, wspace=0.3, hspace=0.5)\n",
    "\n",
    "axes[0,0].set_title('TEMPMED')\n",
    "axes[1,0].set_title('TEMPMIN')\n",
    "axes[2,0].set_title('TEMPMAX')\n",
    "axes[3,0].set_title('PREC')\n",
    "axes[4,0].set_title('PREC_NORM')\n",
    "axes[5,0].set_title('FINSEM')\n",
    "axes[6,0].set_title('TEMP_DIFF')\n",
    "axes[7,0].set_title('TEMP_SUM')\n",
    "axes[8,0].set_title('TEMP_MUL')\n",
    "axes[9,0].set_title('CONSUM')\n",
    "\n",
    "\n",
    "axes[0,0].set_xlabel(None)\n",
    "axes[1,0].set_xlabel(None)\n",
    "axes[2,0].set_xlabel(None)\n",
    "axes[3,0].set_xlabel(None)\n",
    "axes[4,0].set_xlabel(None)\n",
    "axes[5,0].set_xlabel(None)\n",
    "axes[6,0].set_xlabel(None)\n",
    "axes[7,0].set_xlabel(None)\n",
    "axes[8,0].set_xlabel(None)\n",
    "axes[9,0].set_xlabel(None)\n",
    "\n",
    "axes[0,1].set_title('TEMPMED')\n",
    "axes[1,1].set_title('TEMPMIN')\n",
    "axes[2,1].set_title('TEMPMAX')\n",
    "axes[3,1].set_title('PREC')\n",
    "axes[4,1].set_title('PREC_NORM')\n",
    "axes[5,1].set_title('FINSEM')\n",
    "axes[6,1].set_title('TEMP_DIFF')\n",
    "axes[7,1].set_title('TEMP_SUM')\n",
    "axes[8,1].set_title('TEMP_MUL')\n",
    "axes[9,1].set_title('CONSUM')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrem correlacions\n",
    "\n",
    "Mostrem les correlacions entre els atributs numerics.\n",
    "\n",
    "El que ens importa especialment son les correlacions entre l'atribut de consum i la resta d'atributs.\n",
    "\n",
    "Utilitzem el seguent criteri per les correlacions segons el seu valor absolut:\n",
    "* \\[0, 0.3) -> sense correlacio\n",
    "* \\[0.3, 0.5) -> correlacio baixa\n",
    "* \\[0.5, 0.7) -> correlacio mitja\n",
    "* \\[0.7, 0.9) -> correlacio alta\n",
    "* \\[0.9, 1\\] -> correlacio molt alta\n",
    "\n",
    "Veiem les seguents correlacions:\n",
    "* Consum i temperatura mitjana: correlacio mitja\n",
    "* Consum i temperatura minima: correlacio baixa\n",
    "* Consum i temperatura maxima: correlacio mitja\n",
    "* Consum i precipitacio: sense correlacio\n",
    "* Consum i cap de setmana: correlacio mitja\n",
    "      \n",
    "No hi ha cap variable que tingui correlacio alta o molt alta amb el consum.\n",
    "    \n",
    "\n",
    "També hi ha correlacions mitjes o altes entre les diferents mostres de temperatures (minima, mitjana, maxima)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = dataset.corr()\n",
    "plt.figure()\n",
    "\n",
    "ax = sns.heatmap(co, annot=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(figsize=(12, 12))\n",
    "rel = sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fem tests de les distribucions\n",
    "\n",
    "Primer fem test per a veure quins son els attributs amb una distribució normal.\n",
    "\n",
    "Després fem tests amb el atributs no normals per intentar trobar la distribució."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.values\n",
    "attributes = data[:, 1:]\n",
    "\n",
    "for idx, attribute in enumerate(attributes.T):\n",
    "\n",
    "    stat, p = normaltest(attribute)\n",
    "    alpha = .05\n",
    "\n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        print(f\"{dataset.columns[idx + 1]}: The null hypothesis can be rejected\")\n",
    "    else:\n",
    "        print(f\"{dataset.columns[idx + 1]}: The null hypothesis cannot be rejected (comes from a normal dist)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem que només TEMPMED, TEMPMAX, TEMPMITJA passen el test normal i per tant son els unics atributs que tenen una distribució normal.\n",
    "\n",
    "Fem més tests amb kstest per trobar les distribucions que no son normals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distribution(data):\n",
    "    dist_names = [\"norm\", \"expon\", \"exponweib\", \"gamma\", \"weibull_max\", \"weibull_min\", \"pareto\"]\n",
    "    dist_results = []\n",
    "    params = {}\n",
    "    for dist_name in dist_names:\n",
    "        dist = getattr(scipy.stats, dist_name)\n",
    "        param = dist.fit(data)\n",
    "\n",
    "        params[dist_name] = param\n",
    "        # Applying the Kolmogorov-Smirnov test\n",
    "        D, p = kstest(data, dist_name, args=param)\n",
    "        print(\"p value for \"+dist_name+\" = \"+str(p))\n",
    "        dist_results.append((dist_name, p))\n",
    "\n",
    "    # select the best fitted distribution\n",
    "    best_dist, best_p = (max(dist_results, key=lambda item: item[1]))\n",
    "    # store the name of the best fit and its p value\n",
    "\n",
    "    print(\"Best fitting distribution: \"+str(best_dist))\n",
    "    print(\"Best p value: \"+ str(best_p))\n",
    "    print(\"Parameters for the best fit: \"+ str(params[best_dist]))\n",
    "\n",
    "    return best_dist, best_p, params[best_dist]\n",
    "\n",
    "for idx, attribute in enumerate(attributes.T):\n",
    "    attribute = attribute.astype(float)\n",
    "    print()\n",
    "    print(f\"Checking distribution for {dataset.columns[idx + 1]}\")\n",
    "    get_best_distribution(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las distribuciones que encontramos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descartem atributs\n",
    "\n",
    "Com que les precipitacions no tenen correlacio amb el consum, i tampoc segueixen una distribució Gaussiana, el descartem.\n",
    "Com que la temperatura mínima te correlació baixa, la descartem també.\n",
    "I com hem descartat alguns valors, actualitzem els valors a x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset.drop(['DATA', 'PREC', 'PREC_NORM'], axis=1)\n",
    "\n",
    "data = dataset2.values\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer començem creant les funcions de cada regressió per posteriorment poder cridar-les més facilment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APARTADO B\n",
    "\n",
    "import math\n",
    "import numpy as np #importem la llibreria\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet, RANSACRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return ((v1 - v2)**2).mean()\n",
    "\n",
    "def linearReg(x, y):\n",
    "    # Creem un objecte de regressió de sklearn\n",
    "    regr = LinearRegression()\n",
    "    # Entrenem el model per a predir y a partir de x\n",
    "    regr.fit(x, y)\n",
    "    # Retornem el model entrenat\n",
    "    return regr\n",
    "\n",
    "def lassoReg(x, y):\n",
    "    regr = Lasso()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n",
    "\n",
    "def ridgeReg(x, y):\n",
    "    regr = Ridge()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n",
    "\n",
    "def elasticReg(x, y):\n",
    "    regr = ElasticNet()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n",
    "\n",
    "def ransacReg(x, y):\n",
    "    regr = RANSACRegressor()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Després de definir les funcions per als diferents tipus de regressions, hem decidit provar diferents tipus de feature scaling. Aquests sent els dos tipus més utilitzats, anomenats normalització (min-max feature scaling) i estandardització (standard score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_train, minimum=None, maximum=None):\n",
    "    if minimum is None:\n",
    "        minimum = x_train.min(0)\n",
    "    if maximum is None:\n",
    "        maximum = x_train.max(0)\n",
    "\n",
    "    x_t = x_train - minimum\n",
    "    x_t /= maximum - minimum\n",
    "    \n",
    "    return x_t, maximum, minimum\n",
    "\n",
    "def normalize_train_test(x_train, x_test):\n",
    "    x_train, maximum, minimum = normalize(x_train)\n",
    "    x_test, _, _ = normalize(x_test)\n",
    "    \n",
    "    return x_train, x_test, maximum, minimum\n",
    "\n",
    "\n",
    "def standarize(x, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        mean = x.mean(0)\n",
    "    if std is None:\n",
    "        std = x.std(0)\n",
    "    \n",
    "    return (x - mean[None, :]) / std[None, :], mean, std\n",
    "\n",
    "def standarize_train_test(x_train, x_test):\n",
    "    x_train, mean, std = standarize(x_train)\n",
    "    x_test, _, _ = standarize(x_test, mean=mean, std=std)\n",
    "    \n",
    "    return x_train, x_test, mean, std\n",
    "\n",
    "def standarize_y(y, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        mean = y.mean(0)\n",
    "    if std is None:\n",
    "        std = y.std(0)\n",
    "    \n",
    "    return (y - mean) / std, mean, std\n",
    "\n",
    "def standarize_train_test_y(y_train, y_test):\n",
    "    y_train, mean, std = standarize_y(y_train)\n",
    "    y_test, _, _ = standarize_y(y_test, mean=mean, std=std)\n",
    "    \n",
    "    return y_train, y_test, mean, std\n",
    "\n",
    "x = x.astype(float)\n",
    "y = y.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "\n",
    "x_train_std, x_test_std, x_mean, x_std = standarize_train_test(x_train, x_test)\n",
    "y_train_std, y_test_std, y_mean, y_std = standarize_train_test_y(y_train, y_test)\n",
    "\n",
    "\n",
    "x_train_norm, x_test_norm, x_max, x_min = normalize_train_test(x_train, x_test)\n",
    "y_train_norm, y_test_norm, y_max, y_min = normalize_train_test(y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "\n",
    "print(\"Linear Regression sense normalitzar:\")\n",
    "print(\"Error MSE: %f\" % (error))\n",
    "print(\"R2 score: %f\" % (r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "\n",
    "regr = linearReg(x_train_norm, y_train_norm)\n",
    "error = mse(y_test_norm, regr.predict(x_test_norm)) # calculem error\n",
    "r2 = r2_score(y_test_norm, regr.predict(x_test_norm))\n",
    "kfold_score = cross_val_score(regr, x_train_norm, y_train_norm, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression normalized:\")\n",
    "print(\"Error MSE: %f\" % (error))\n",
    "print(\"R2 score: %f\" % (r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "\n",
    "regr = linearReg(x_train_std, y_train_std)\n",
    "error = mse(y_test_std, regr.predict(x_test_std)) # calculem error\n",
    "r2 = r2_score(y_test_std, regr.predict(x_test_std))\n",
    "kfold_score = cross_val_score(regr, x_train_std, y_train_std, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression standarized:\")\n",
    "print(\"Error MSE: %f\" % (error))\n",
    "print(\"R2 score: %f\" % (r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com es pot veure als resultats anteriors, la majoria de vegades que hem executat el codi ens ha donat un major r2 score i r2 score k-fold al utilitzar la esandardització, per lo tant per el que queda de projecte utilitzarem aquesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = x_train_std, x_test_std, y_train_std, y_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Linear Regression per atributs:\")\n",
    "print(*dataset2.columns[:-1])\n",
    "for i in range(len(dataset2.columns[:-1])):\n",
    "    x_t = x_train[:,i] # seleccionem atribut i en conjunt de train\n",
    "    x_v = x_test[:,i] # seleccionem atribut i en conjunt de test.\n",
    "    x_t = np.reshape(x_t,(x_t.shape[0],1))\n",
    "    x_v = np.reshape(x_v,(x_v.shape[0],1))\n",
    "\n",
    "    regr = linearReg(x_t, y_train_std)    \n",
    "    error = mse(y_test, regr.predict(x_v)) # calculem error\n",
    "    r2 = r2_score(y_test, regr.predict(x_v))\n",
    "    kfold_score = cross_val_score(regr, x_t, y_train, scoring='r2', cv=5).mean()\n",
    "\n",
    "    print()\n",
    "    print(\"Error en atribut %s: %f\" %(dataset2.columns[i], error))\n",
    "    print(\"R2 score en atribut %s: %f\" %(dataset2.columns[i], r2))\n",
    "    print(\"R2 score (K-Fold) en atribut %s: %f\" % (dataset2.columns[i], kfold_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "std_x = standarize(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elecció del tipus de regressió\n",
    "\n",
    "A continuació, provarem diferents tipus de regressió per comprovar quina és la que s'adapta millor al nostre projecte. Les regressions que provarem inicialment i que ja hem definit al principi d'aquest apartat son: Regressió Lineal, Regressió Lasso, Regressió Ridge, Regressió ElasticNet, i Regressió Ransac. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = lassoReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Lasso:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = ridgeReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Ridge:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = elasticReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"ElasticNet:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = ransacReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"RANSAC:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidament, calcularem el PCA per saber quin és el nombre d'atributs idoni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "mses, r2s = [], []\n",
    "for i in range(1, x_train.shape[1]):\n",
    "    pca = PCA(n_components=i)\n",
    "    x_train_transformed = pca.fit_transform(x_train)\n",
    "    x_test_transformed = pca.transform(x_test)\n",
    "\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(x_train_transformed, y_train)\n",
    "    preds = linear_model.predict(x_test_transformed)\n",
    "\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    kfold_score = cross_val_score(linear_model, x_train_transformed, y_train, scoring='r2', cv=5).mean()\n",
    "    print(f\"PCA_{i} - MSE: {mse:.3f}; R2: {r2:.3f}; R2 K-Fold: {kfold_score:.3f}\")\n",
    "    \n",
    "    mses.append(mse)\n",
    "    r2s.append(r2)\n",
    "    \n",
    "plt.plot(mses, label='mse')\n",
    "plt.plot(r2s, label='r2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com es pot veure al resultat, el nombre d'atributs idoni es 2-3 ja que es quan el mean squared error i el r2 score arriben a un punt on, o no milloren més, o milloren tant poc que no es rentable ja que com més atributs utilitza, més lenta és la regressió. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresió polinomial\n",
    "\n",
    "Després de provar les regressions lineals anteriors, ara provarem amb regressions polinomials de grau 2 i 3 per veure si el nostre cas necessita una funció amb corva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = linearReg(x_train_poly, y_train)\n",
    "\n",
    "error = mean_squared_error(y_test, regr.predict(x_test_poly)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test_poly))\n",
    "kfold_score = cross_val_score(regr, x_train_poly, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"PloynomialRegression (degree = 2):\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = linearReg(x_train_poly, y_train)\n",
    "\n",
    "error = mean_squared_error(y_test, regr.predict(x_test_poly)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test_poly))\n",
    "kfold_score = cross_val_score(regr, x_train_poly, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"PloynomialRegression (degree = 3):\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ens empitjora el resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat A\n",
    "\n",
    "A continuació hem creat una classe per a poder fer un regressor lineal de zero, sense tenir que cridar a la funció de sklearn com hem fet amb anterioritat. Aquesta classe ens retorna el mean squared error, el r2 score, i el nombre d'iteracions que ha necessitat, per així poder comparar el seu rendiment amb altres classes incluïda la de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APARTADO A\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Regressor(object):\n",
    "    def __init__(self, w0, w1, alpha):\n",
    "        # Inicialitzem w0 i w1 (per ser ampliat amb altres w's)\n",
    "        self.w0 = w0\n",
    "        self.w1 = w1\n",
    "        self.alpha = alpha\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # implementar aqui la funció de prediccio\n",
    "        pass\n",
    "    \n",
    "    def __update(self, hy, y):\n",
    "        # actualitzar aqui els pesos donada la prediccio (hy) i la y real.\n",
    "        self.weights = self.weights - self.alpha / m * (X.transpose() * (X * self.weights - y));\n",
    "    \n",
    "    def train(self, max_iter, epsilon):\n",
    "        # Entrenar durant max_iter iteracions o fins que la millora sigui inferior a epsilon\n",
    "        iterations = 0            \n",
    "        pass\n",
    "    \n",
    "    \n",
    "class RegressorLineal(object):\n",
    "    def __init__(self, w0, weights, alpha):\n",
    "        # Inicialitzem w0 i w1 (per ser ampliat amb altres w's)\n",
    "        self.w0 = w0\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.iterations_to_converge = 0\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # implementar aqui la funció de prediccio\n",
    "        return np.dot(x, self.weights) + self.w0\n",
    "    \n",
    "    def __update(self, x, hy, y):\n",
    "        # actualitzar aqui els pesos donada la prediccio (hy) i la y real.\n",
    "        w0_diff, weight_diffs = self.__gradients(x, y, hy)\n",
    "        self.__update_weights(w0_diff, weight_diffs)\n",
    "        \n",
    "    def __gradients(self, x, real_y, predicted_y):\n",
    "        error = predicted_y - real_y\n",
    "        \n",
    "        w0_gradient = (1 / real_y.shape[0]) * np.sum(error)\n",
    "        weights_gradients = (1 / real_y.shape[0]) * np.dot(x.transpose(), error)\n",
    "        \n",
    "        return w0_gradient, weights_gradients\n",
    "        \n",
    "    def __update_weights(self, w0_diff, weight_diffs):\n",
    "        self.w0 = self.w0 - self.alpha * w0_diff\n",
    "        self.weights = self.weights - self.alpha * weight_diffs\n",
    "    \n",
    "    def train(self, x, y, max_iter, epsilon):\n",
    "        # Entrenar durant max_iter iteracions o fins que la millora sigui inferior a epsilon\n",
    "        iterations = 0\n",
    "        self.iterations_to_converge = 0\n",
    "        old_w0 = self.w0\n",
    "        old_weights = self.weights.copy()\n",
    "        while iterations < max_iter:\n",
    "            self.iterations_to_converge += 1\n",
    "            predicted_y = self.predict(x)\n",
    "            \n",
    "            self.__update(x, predicted_y, y)\n",
    "    \n",
    "            if abs(self.w0 - old_w0) < epsilon and np.allclose(self.weights, old_weights, atol=epsilon):\n",
    "                return\n",
    "            \n",
    "            old_w0 = self.w0\n",
    "            old_weights = self.weights\n",
    "            iterations += 1\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.train(x, y, 10000, 0.001)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(w0=self.w0, weights=self.weights, alpha=self.alpha)\n",
    "\n",
    "\n",
    "    \n",
    "regr = RegressorLineal(1, np.ones(len(x_train[0]), dtype=float), 0.1)\n",
    "\n",
    "regr.train(x_train, y_train, 100000, 0.001)\n",
    "\n",
    "y_predict = regr.predict(x_test)\n",
    "\n",
    "error = mean_squared_error(y_test, y_predict) # calculem error\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "print(\"CUSTOM Linear Regression:\")\n",
    "print(f\"Iterations to converge: {regr.iterations_to_converge}\")\n",
    "print(\"Error: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També hem creat una classe per el regressor del tipus Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegressorRidge(object):\n",
    "    def __init__(self, w0, weights, alpha, l2_penalty):\n",
    "        # Inicialitzem w0 i w1 (per ser ampliat amb altres w's)\n",
    "        self.w0 = w0\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        self.l2_penalty = l2_penalty\n",
    "        \n",
    "        self.iterations_to_converge = 0\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # implementar aqui la funció de prediccio\n",
    "        return np.dot(x, self.weights) + self.w0\n",
    "    \n",
    "    def __update(self, x, hy, y):\n",
    "        # actualitzar aqui els pesos donada la prediccio (hy) i la y real.\n",
    "        w0_diff, weight_diffs = self.__gradients(x, y, hy)\n",
    "        self.__update_weights(w0_diff, weight_diffs)\n",
    "        \n",
    "    def __gradients(self, x, real_y, predicted_y):\n",
    "        error = predicted_y - real_y\n",
    "        \n",
    "        w0_gradient = (1 / real_y.shape[0]) * np.sum(error)\n",
    "        \n",
    "        # DIFERENCIA AMB EL REGRESOR LINEAL NORMAL\n",
    "        weights_gradients = (1 / real_y.shape[0]) * (-(2 * x.transpose().dot(real_y - predicted_y)) +               \n",
    "               (2 * self.l2_penalty * self.weights)) # apliquem el l2_penalty del Ridge\n",
    "        \n",
    "        return w0_gradient, weights_gradients\n",
    "        \n",
    "    def __update_weights(self, w0_diff, weight_diffs):\n",
    "        self.w0 = self.w0 - self.alpha * w0_diff\n",
    "        self.weights = self.weights - self.alpha * weight_diffs\n",
    "    \n",
    "    def train(self, x, y, max_iter, epsilon):\n",
    "        # Entrenar durant max_iter iteracions o fins que la millora sigui inferior a epsilon\n",
    "        iterations = 0\n",
    "        self.iterations_to_converge = 0\n",
    "        old_w0 = self.w0\n",
    "        old_weights = self.weights.copy()\n",
    "        while iterations < max_iter:\n",
    "            self.iterations_to_converge += 1\n",
    "            predicted_y = self.predict(x)\n",
    "            \n",
    "            self.__update(x, predicted_y, y)\n",
    "    \n",
    "            if abs(self.w0 - old_w0) < epsilon and np.allclose(self.weights, old_weights, atol=epsilon):\n",
    "                return\n",
    "            \n",
    "            old_w0 = self.w0\n",
    "            old_weights = self.weights\n",
    "            iterations += 1\n",
    "            \n",
    "    def fit(self, x, y):\n",
    "        self.train(x, y, 10000, 0.001)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(w0=self.w0, weights=self.weights, alpha=self.alpha, l2_penalty=self.l2_penalty)\n",
    "\n",
    "            \n",
    "regr = RegressorRidge(1, np.ones(len(x_train[0]), dtype=float), 0.1, 100)\n",
    "\n",
    "regr.train(x_train, y_train, 100000, 0.001)\n",
    "\n",
    "y_predict = regr.predict(x_test)\n",
    "\n",
    "\n",
    "error = mean_squared_error(y_test, y_predict) # calculem error\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "print(\"CUSTOM Ridge Regression:\")\n",
    "print(f\"Iterations to converge: {regr.iterations_to_converge}\")\n",
    "print(\"Error: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com es pot veure en els dos resultats anteriors, la classe del Regressor Ridge i la del Regressor lineal ens donen resultats semblants, encara que el Regressor Ridge ho fa en un nombre menor de iteracions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalment, reutilitzant la classe RegressorRidge que hem creat prèviament, hem intentat fer regressions polinomials de grau 2 i 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = RegressorRidge(1, np.ones(len(x_train_poly[0]), dtype=float), 0.1, 10)\n",
    "\n",
    "error = mean_squared_error(y_test, regr.predict(x_test_poly)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test_poly))\n",
    "kfold_score = cross_val_score(regr, x_train_poly, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"PloynomialRegression (degree = 2):\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = RegressorRidge(1, np.ones(len(x_train_poly[0]), dtype=float), 0.1, 10)\n",
    "\n",
    "error = mean_squared_error(y_test, regr.predict(x_test_poly)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test_poly))\n",
    "kfold_score = cross_val_score(regr, x_train_poly, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"PloynomialRegression (degree = 3):\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RegressorLineal(1, np.ones(len(x_train[0]), dtype=float), 0.1)\n",
    "\n",
    "regr.train(x_train, y_train, 100000, 0.001)\n",
    "\n",
    "y_predict = regr.predict(x_test)\n",
    "\n",
    "error = mean_squared_error(y_test, y_predict) # calculem error\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "print(\"CUSTOM Linear Regression:\")\n",
    "print(f\"Iterations to converge: {regr.iterations_to_converge}\")\n",
    "print(\"Error: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "\n",
    "\n",
    "sns.regplot(x=y_test, y=y_predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e49f01b25252117987057767e4860399151593b17ce2349b8fe42cec150c223d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
