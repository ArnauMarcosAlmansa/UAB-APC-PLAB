{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat C\n",
    "\n",
    "## Setup\n",
    "\n",
    "* Importem les llibreries\n",
    "* Configurem pandas\n",
    "* Importem el dataset\n",
    "* Inspeccionem les dimensions de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APARTADO C\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import normaltest, kstest, boxcox\n",
    "\n",
    "# Visualitzarem 4 decimals per mostra\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "dataset = pd.read_csv(\"Consumo_cerveja.txt\", header=0, delimiter=',', decimal=\".\", names=[\"DATA\",\"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\",\"FINSEM\",\"CONSUM\"])\n",
    "dataset = dataset.assign(TEMP_DIFF= dataset.TEMPMAX - dataset.TEMPMIN)\n",
    "dataset = dataset.assign(TEMP_SUM= (dataset.TEMPMAX + dataset.TEMPMIN + dataset.TEMPMED))\n",
    "dataset = dataset.assign(TEMP_MUL= (dataset.TEMPMAX * dataset.TEMPMIN * dataset.TEMPMED))\n",
    "\n",
    "'''\n",
    "dataset = dataset.assign(MONTH = dataset.DATA.apply(lambda data: data.split(\"-\")[1]))\n",
    "dataset = dataset.assign(JANUARY = dataset.MONTH.apply(lambda month: 1 if month == '01' else 0))\n",
    "dataset = dataset.assign(FEBRUARY = dataset.MONTH.apply(lambda month: 1 if month == '02' else 0))\n",
    "dataset = dataset.assign(MARCH = dataset.MONTH.apply(lambda month: 1 if month == '03' else 0))\n",
    "dataset = dataset.assign(APRIL = dataset.MONTH.apply(lambda month: 1 if month == '04' else 0))\n",
    "dataset = dataset.assign(MAY = dataset.MONTH.apply(lambda month: 1 if month == '05' else 0))\n",
    "dataset = dataset.assign(JUNE = dataset.MONTH.apply(lambda month: 1 if month == '06' else 0))\n",
    "dataset = dataset.assign(JULY = dataset.MONTH.apply(lambda month: 1 if month == '07' else 0))\n",
    "dataset = dataset.assign(AUGUST = dataset.MONTH.apply(lambda month: 1 if month == '08' else 0))\n",
    "dataset = dataset.assign(SEPTEMBER = dataset.MONTH.apply(lambda month: 1 if month == '09' else 0))\n",
    "dataset = dataset.assign(OCTOBER = dataset.MONTH.apply(lambda month: 1 if month == '10' else 0))\n",
    "dataset = dataset.assign(NOVEMBER = dataset.MONTH.apply(lambda month: 1 if month == '11' else 0))\n",
    "dataset = dataset.assign(DECEMBER = dataset.MONTH.apply(lambda month: 1 if month == '12' else 0))\n",
    "dataset = dataset.assign(MONTH = dataset.MONTH.apply(lambda month: int(month)))\n",
    "'''\n",
    "\n",
    "prec_norm, _ = boxcox(dataset[\"PREC\"] + 0.01)\n",
    "dataset = dataset.assign(PREC_NORM=prec_norm)\n",
    "#dataset = dataset[[\"DATA\",\"MONTH\", \"JANUARY\", \"FEBRUARY\", \"MARCH\", \"APRIL\", \"MAY\", \"JUNE\", \"JULY\", \"AUGUST\", \"SEPTEMBER\", \"OCTOBER\", \"NOVEMBER\", \"DECEMBER\",  \"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\", \"PREC_NORM\",\"FINSEM\",\"TEMP_DIFF\", \"TEMP_SUM\", \"TEMP_MUL\", \"CONSUM\"]]\n",
    "dataset = dataset[[\"DATA\", \"TEMPMED\",\"TEMPMIN\",\"TEMPMAX\",\"PREC\", \"PREC_NORM\",\"FINSEM\",\"TEMP_DIFF\", \"TEMP_SUM\", \"TEMP_MUL\", \"CONSUM\"]]\n",
    "data = dataset.values\n",
    "x = data[:, 1:-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "print(\"Dimensionalitat de la BBDD:\", dataset.shape)\n",
    "print(\"Dimensionalitat de les entrades X\", x.shape)\n",
    "print(\"Dimensionalitat de l'atribut Y\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrem les característiques del dataset\n",
    "\n",
    "* Número d'entrades\n",
    "* Descripció de les columnes\n",
    "    * Nom de cada columna\n",
    "    * Quantitat de registres amb valor\n",
    "    * Si pot ser null o no\n",
    "    * Tipus de dada\n",
    "\n",
    "La part del tipus de dada es especialment important. Si les dades no s'importen com a números no podrem treballar correctament amb el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busquem valors nulls\n",
    "\n",
    "No n'hi ha cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per comptar el nombre de valors no existents:\")\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualitzem els primers registres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per visualitzar les primeres 5 mostres de la BBDD:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per veure estadístiques dels atributs numèrics de la BBDD:\")\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrem correlacions\n",
    "\n",
    "Mostrem les correlacions entre els atributs numerics.\n",
    "\n",
    "El que ens importa especialment son les correlacions entre l'atribut de consum i la resta d'atributs.\n",
    "\n",
    "Utilitzem el seguent criteri per les correlacions segons el seu valor absolut:\n",
    "* \\[0, 0.3) -> sense correlacio\n",
    "* \\[0.3, 0.5) -> correlacio baixa\n",
    "* \\[0.5, 0.7) -> correlacio mitja\n",
    "* \\[0.7, 0.9) -> correlacio alta\n",
    "* \\[0.9, 1\\] -> correlacio molt alta\n",
    "\n",
    "Veiem les seguents correlacions:\n",
    "* Consum i temperatura mitjana: correlacio mitja\n",
    "* Consum i temperatura minima: correlacio baixa\n",
    "* Consum i temperatura maxima: correlacio mitja\n",
    "* Consum i precipitacio: sense correlacio\n",
    "* Consum i cap de setmana: correlacio mitja\n",
    "      \n",
    "No hi ha cap variable que tingui correlacio alta o molt alta amb el consum.\n",
    "    \n",
    "\n",
    "També hi ha correlacions mitjes o altes entre les diferents mostres de temperatures (minima, mitjana, maxima)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = dataset.corr()\n",
    "plt.figure()\n",
    "\n",
    "ax = sns.heatmap(co, annot=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(figsize=(12, 12))\n",
    "rel = sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fem tests de les distribucions\n",
    "\n",
    "Primer fem test per a veure quins son els attributs amb una distribució normal.\n",
    "\n",
    "Després fem tests amb el atributs no normals per intentar trobar la distribució."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.values\n",
    "attributes = data[:, 1:]\n",
    "\n",
    "for idx, attribute in enumerate(attributes.T):\n",
    "\n",
    "    stat, p = normaltest(attribute)\n",
    "    alpha = .05\n",
    "\n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        print(f\"{dataset.columns[idx + 1]}: The null hypothesis can be rejected\")\n",
    "    else:\n",
    "        print(f\"{dataset.columns[idx + 1]}: The null hypothesis cannot be rejected (comes from a normal dist)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem que només TEMPMED i TEMPMAX passen el test normal i per tant son els unics atributs que tenen una distribució normal.\n",
    "\n",
    "Fem més tests amb kstest per trobar les distribucions que no son normals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distribution(data):\n",
    "    dist_names = [\"norm\", \"expon\", \"exponweib\", \"gamma\", \"weibull_max\", \"weibull_min\", \"pareto\"]\n",
    "    dist_results = []\n",
    "    params = {}\n",
    "    for dist_name in dist_names:\n",
    "        dist = getattr(scipy.stats, dist_name)\n",
    "        param = dist.fit(data)\n",
    "\n",
    "        params[dist_name] = param\n",
    "        # Applying the Kolmogorov-Smirnov test\n",
    "        D, p = kstest(data, dist_name, args=param)\n",
    "        print(\"p value for \"+dist_name+\" = \"+str(p))\n",
    "        dist_results.append((dist_name, p))\n",
    "\n",
    "    # select the best fitted distribution\n",
    "    best_dist, best_p = (max(dist_results, key=lambda item: item[1]))\n",
    "    # store the name of the best fit and its p value\n",
    "\n",
    "    print(\"Best fitting distribution: \"+str(best_dist))\n",
    "    print(\"Best p value: \"+ str(best_p))\n",
    "    print(\"Parameters for the best fit: \"+ str(params[best_dist]))\n",
    "\n",
    "    return best_dist, best_p, params[best_dist]\n",
    "\n",
    "for idx, attribute in enumerate(attributes.T):\n",
    "    attribute = attribute.astype(float)\n",
    "    print()\n",
    "    print(f\"Checking distribution for {dataset.columns[idx + 1]}\")\n",
    "    get_best_distribution(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las distribuciones que encontramos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descartem atributs\n",
    "\n",
    "Com que les precipitacions no tenen correlacio amb el consum, i tampoc segueixen una distribució Gaussiana, el descartem.\n",
    "Com que la temperatura mínima te correlació baixa, la descartem també.\n",
    "I com hem descartat alguns valors, actualitzem els valors a x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset.drop(['DATA'], axis=1)\n",
    "\n",
    "data = dataset2.values\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APARTADO B\n",
    "\n",
    "import math\n",
    "import numpy as np #importem la llibreria\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet, RANSACRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return ((v1 - v2)**2).mean()\n",
    "\n",
    "def mean_quare_error(v1, v2):\n",
    "    return ((v1 - v2)**2).mean()\n",
    "\n",
    "def linearReg(x, y):\n",
    "    # Creem un objecte de regressió de sklearn\n",
    "    regr = LinearRegression()\n",
    "    # Entrenem el model per a predir y a partir de x\n",
    "    regr.fit(x, y)\n",
    "    # Retornem el model entrenat\n",
    "    return regr\n",
    "\n",
    "def lassoReg(x, y):\n",
    "    regr = Lasso()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n",
    "\n",
    "def ridgeReg(x, y):\n",
    "    regr = Ridge()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n",
    "\n",
    "def elasticReg(x, y):\n",
    "    regr = ElasticNet()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n",
    "\n",
    "def ransacReg(x, y):\n",
    "    regr = RANSACRegressor()\n",
    "    regr.fit(x, y)\n",
    "    return regr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_train):\n",
    "    minimum = x_train.min(0)\n",
    "    maximum = x_train.max(0)\n",
    "    x_t = x_train - minimum \n",
    "    x_t /= maximum - minimum\n",
    "    return x_t\n",
    "\n",
    "def standarize(x_train):\n",
    "    mean = x_train.mean(0)\n",
    "    std = x_train.std(0)\n",
    "    x_t = x_train - mean[None, :]\n",
    "    x_t /= std[None, :]\n",
    "    return x_t\n",
    "\n",
    "def standarize_y(y):\n",
    "    mean = y.mean()\n",
    "    std = y.std()\n",
    "    y_t = y - mean\n",
    "    y_t /= std\n",
    "    return y_t\n",
    "\n",
    "x = x.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "no_std_x = x.copy()\n",
    "norm_x = normalize(x)\n",
    "std_x = standarize(x)\n",
    "\n",
    "no_std_y = y.copy()\n",
    "norm_y = normalize(y)\n",
    "std_y = standarize_y(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(no_std_x, no_std_y, test_size=0.2)\n",
    "\n",
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "\n",
    "print(\"Linear Regression sense normalitzar:\")\n",
    "print(\"Error MSE: %f\" % (error))\n",
    "print(\"R2 score: %f\" % (r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(norm_x, norm_y, test_size=0.2)\n",
    "\n",
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression normalized:\")\n",
    "print(\"Error MSE: %f\" % (error))\n",
    "print(\"R2 score: %f\" % (r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(std_x, std_y, test_size=0.2)\n",
    "\n",
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression standarized:\")\n",
    "print(\"Error MSE: %f\" % (error))\n",
    "print(\"R2 score: %f\" % (r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(std_x, std_y, test_size=0.2)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"Linear Regression per atributs:\")\n",
    "print(*dataset2.columns[:-1])\n",
    "for i in range(len(dataset2.columns[:-1])):\n",
    "    x_t = x_train[:,i] # seleccionem atribut i en conjunt de train\n",
    "    x_v = x_test[:,i] # seleccionem atribut i en conjunt de test.\n",
    "    x_t = np.reshape(x_t,(x_t.shape[0],1))\n",
    "    x_v = np.reshape(x_v,(x_v.shape[0],1))\n",
    "\n",
    "    regr = linearReg(x_t, y_train)    \n",
    "    error = mse(y_test, regr.predict(x_v)) # calculem error\n",
    "    r2 = r2_score(y_test, regr.predict(x_v))\n",
    "    kfold_score = cross_val_score(regr, x_t, y_train, scoring='r2', cv=5).mean()\n",
    "\n",
    "    print()\n",
    "    print(\"Error en atribut %s: %f\" %(dataset2.columns[i], error))\n",
    "    print(\"R2 score en atribut %s: %f\" %(dataset2.columns[i], r2))\n",
    "    print(\"R2 score (K-Fold) en atribut %s: %f\" % (dataset2.columns[i], kfold_score))\n",
    "\n",
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = lassoReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Lasso:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = ridgeReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Ridge:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = elasticReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"ElasticNet:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = ransacReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"RANSAC:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = dataset.drop(['DATA', 'PREC'], axis=1)\n",
    "\n",
    "data = dataset3.values\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "x = x.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "std_x = standarize(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linearReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = lassoReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Lasso:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = ridgeReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"Ridge:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = elasticReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"ElasticNet:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "regr = ransacReg(x_train, y_train)\n",
    "error = mse(y_test, regr.predict(x_test)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test))\n",
    "kfold_score = cross_val_score(regr, x_train, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"RANSAC:\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(std_x, std_y, test_size=0.2)\n",
    "\n",
    "mses, r2s = [], []\n",
    "for i in range(1, x_train.shape[1]):\n",
    "    pca = PCA(n_components=i)\n",
    "    x_train_transformed = pca.fit_transform(x_train)\n",
    "    x_test_transformed = pca.transform(x_test)\n",
    "\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(x_train_transformed, y_train)\n",
    "    preds = linear_model.predict(x_test_transformed)\n",
    "\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    kfold_score = cross_val_score(linear_model, x_train_transformed, y_train, scoring='r2', cv=5).mean()\n",
    "    print(f\"PCA_{i} - MSE: {mse:.3f}; R2: {r2:.3f}; R2 K-Fold: {kfold_score:.3f}\")\n",
    "    \n",
    "    mses.append(mse)\n",
    "    r2s.append(r2)\n",
    "    \n",
    "plt.plot(mses, label='mse')\n",
    "plt.plot(r2s, label='r2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresió polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(std_x, std_y, test_size=0.2)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = linearReg(x_train_poly, y_train)\n",
    "\n",
    "error = mean_squared_error(y_test, regr.predict(x_test_poly)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test_poly))\n",
    "kfold_score = cross_val_score(regr, x_train_poly, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"PloynomialRegression (degree = 2):\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = linearReg(x_train_poly, y_train)\n",
    "\n",
    "error = mean_squared_error(y_test, regr.predict(x_test_poly)) # calculem error\n",
    "r2 = r2_score(y_test, regr.predict(x_test_poly))\n",
    "kfold_score = cross_val_score(regr, x_train_poly, y_train, scoring='r2', cv=5).mean()\n",
    "print()\n",
    "print(\"PloynomialRegression (degree = 3):\")\n",
    "print(\"Error MSE: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n",
    "print(\"R2 score (K-Fold): %f\" % (kfold_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ens empitjora el resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APARTADO A\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Regressor(object):\n",
    "    def __init__(self, w0, w1, alpha):\n",
    "        # Inicialitzem w0 i w1 (per ser ampliat amb altres w's)\n",
    "        self.w0 = w0\n",
    "        self.w1 = w1\n",
    "        self.alpha = alpha\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # implementar aqui la funció de prediccio\n",
    "        pass\n",
    "    \n",
    "    def __update(self, hy, y):\n",
    "        # actualitzar aqui els pesos donada la prediccio (hy) i la y real.\n",
    "        self.weights = self.weights - self.alpha / m * (X.transpose() * (X * self.weights - y));\n",
    "    \n",
    "    def train(self, max_iter, epsilon):\n",
    "        # Entrenar durant max_iter iteracions o fins que la millora sigui inferior a epsilon\n",
    "        iterations = 0            \n",
    "        pass\n",
    "    \n",
    "    \n",
    "class RegressorProba(object):\n",
    "    def __init__(self, w0, weights, alpha):\n",
    "        # Inicialitzem w0 i w1 (per ser ampliat amb altres w's)\n",
    "        self.w0 = w0\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # implementar aqui la funció de prediccio\n",
    "        return np.dot(x, self.weights) + self.w0\n",
    "    \n",
    "    def __update(self, X, y):\n",
    "        # actualitzar aqui els pesos donada la prediccio (hy) i la y real.\n",
    "        m = len(y)\n",
    "        self.weights = self.weights - self.alpha / m * (X.transpose() * (X * self.weights - y));\n",
    "        \n",
    "    def __gradients(self, x, real_y, predicted_y):\n",
    "        error = predicted_y - real_y\n",
    "        w0_gradient = (1 / len(real_y)) * np.sum(error)\n",
    "        weights_gradients = (1 / len(real_y)) * np.matmul(x.transpose(), error)\n",
    "        \n",
    "        return w0_gradient, weights_gradients\n",
    "        \n",
    "    def __update_weights(self, w0_diff, weight_diffs):\n",
    "        self.w0 = self.w0 - self.alpha * w0_diff\n",
    "        self.weights = self.weights - self.alpha * weight_diffs\n",
    "    \n",
    "    def train(self, x, y, max_iter, epsilon):\n",
    "        # Entrenar durant max_iter iteracions o fins que la millora sigui inferior a epsilon\n",
    "        iterations = 0\n",
    "        old_w0 = self.w0\n",
    "        old_weights = self.weights.copy()\n",
    "        while iterations < max_iter:\n",
    "            predicted_y = self.predict(x)\n",
    "            \n",
    "            w0_diff, weight_diffs = self.__gradients(x, y, predicted_y)\n",
    "                        \n",
    "            self.__update_weights(w0_diff, weight_diffs)\n",
    "    \n",
    "            if abs(self.w0 - old_w0) < epsilon and np.allclose(self.weights, old_weights, atol=epsilon):\n",
    "                return\n",
    "            \n",
    "            old_w0 = self.w0\n",
    "            old_weights = self.weights\n",
    "            iterations += 1\n",
    "\n",
    "            \n",
    "regr = RegressorProba(1, np.ones(len(x_train[0]), dtype=float), 0.1)\n",
    "\n",
    "regr.train(x_train, y_train, 100000, 0.001)\n",
    "\n",
    "y_predict = regr.predict(x_test)\n",
    "\n",
    "error = mean_squared_error(y_test, y_predict) # calculem error\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "print(\"CUSTOM Linear Regression:\")\n",
    "print(\"Error: %f\" %( error))\n",
    "print(\"R2 score: %f\" %( r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegressorRidge(object):\n",
    "    def __init__(self, w0, weights, alpha):\n",
    "        # Inicialitzem w0 i w1 (per ser ampliat amb altres w's)\n",
    "        self.w0 = w0\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # implementar aqui la funció de prediccio\n",
    "        return np.dot(x, self.weights) + self.w0\n",
    "    \n",
    "    def __update(self, X, y):\n",
    "        # actualitzar aqui els pesos donada la prediccio (hy) i la y real.\n",
    "        m = len(y)\n",
    "        self.weights = self.weights - self.alpha / m * (X.transpose() * (X * self.weights - y));\n",
    "        \n",
    "    def __gradients(self, x, real_y, predicted_y):\n",
    "        error = predicted_y - real_y\n",
    "        w0_gradient = (1 / len(real_y)) * np.sum(error)\n",
    "        weights_gradients = (1 / len(real_y)) * np.matmul(x.transpose(), error)\n",
    "        \n",
    "        return w0_gradient, weights_gradients\n",
    "        \n",
    "    def __update_weights(self, w0_diff, weight_diffs):\n",
    "        self.w0 = self.w0 - self.alpha * w0_diff\n",
    "        self.weights = self.weights - self.alpha * weight_diffs\n",
    "    \n",
    "    def train(self, x, y, max_iter, epsilon):\n",
    "        # Entrenar durant max_iter iteracions o fins que la millora sigui inferior a epsilon\n",
    "        iterations = 0\n",
    "        old_w0 = self.w0\n",
    "        old_weights = self.weights.copy()\n",
    "        while iterations < max_iter:\n",
    "            predicted_y = self.predict(x)\n",
    "            \n",
    "            w0_diff, weight_diffs = self.__gradients(x, y, predicted_y)\n",
    "                        \n",
    "            self.__update_weights(w0_diff, weight_diffs)\n",
    "    \n",
    "            if abs(self.w0 - old_w0) < epsilon and np.allclose(self.weights, old_weights, atol=epsilon):\n",
    "                return\n",
    "            \n",
    "            old_w0 = self.w0\n",
    "            old_weights = self.weights\n",
    "            iterations += 1\n",
    "\n",
    "            \n",
    "regr = RegressorRidge(1, np.ones(len(x_train[0]), dtype=float), 0.1)\n",
    "\n",
    "regr.train(x_train, y_train, 100000, 0.001)\n",
    "\n",
    "y_predict = regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e49f01b25252117987057767e4860399151593b17ce2349b8fe42cec150c223d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
