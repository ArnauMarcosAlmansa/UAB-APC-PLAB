{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seguimiento de la practica 1\n",
    "\n",
    "En este juypyter se explican dudas que han sido frecuentes en varios grupos asi como ejemplos de pruebas y/o visualizaciones que podeis hacer.\n",
    "Como se ha mencionado en clase, usad lo que os haga falta acorde a vuestros datos y ampliad el contenido mas allá de lo que se explica en el guión y en este notebook\n",
    "\n",
    "La base de datos que se usa en estos ejemplos trata los datos de una aseguradora sobre sus asegurados y el objetivo es predecir el coste que le supone a la empresa tener asegurado a cada persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante!\n",
    "### El material del jupyter anterior es un \"Guión de practicas\". A partir de los ejemplos que se os facilitan, teneis que seguir desarrollando la practica de forma autonoma\n",
    "\n",
    "#### Ejemplos que os pueden ser de ayuda para empezar a ampliar la practica\n",
    "\n",
    " - Apartado C: Atributos derivados que os aporten mas información (at1 * at2 =at3); tratamiento de datos categoricos; gestion de nans; gestion de desbalanceo de datos (oversampling y undersampling); ...\n",
    " - Apartado B: Regresiones multivariadas, polinomiales, PCA, algoritmos de robustez contra outliers (RANSAC); ...\n",
    " - Apartado A: Regularizadores; momentum; ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('follow_insurance.csv')\n",
    "idx = df.sample(frac=0.7).index\n",
    "\n",
    "train_df = df[df.index.isin(idx)]\n",
    "test_df = df[~df.index.isin(idx)]\n",
    "\n",
    "\n",
    "train_df_x = train_df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']]\n",
    "train_df_y = train_df['expenses']\n",
    "\n",
    "test_df_x = test_df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']]\n",
    "test_df_y = test_df['expenses']\n",
    "\n",
    "print(df.columns)\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirar por encima los datos (tipos, valores, ....)\n",
    "for i in train_df.columns:\n",
    "    if train_df[i].dtype == 'object':\n",
    "        print(f\"{i} \\nNaNs: {train_df[i].isna().sum()}\\nValues: {train_df[i].unique()}\", )\n",
    "    else:\n",
    "        print(f\"{i} \\nNaNs: {train_df[i].isna().sum()}\\nMean_std: {train_df[i].mean():.3f} {train_df[i].std():.3f}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables categoricas no las podeis usar tal cual estan en la base de datos y para poder trabajar con ellas las teneis que convertir a numericas:\n",
    " - Nominales: No hay ninguna relacion aparente entre los diferentes valores que puede tener la variable (Lista de accesorios disponibles en un coche)\n",
    "     - One-hot encoding para las categoricas nominales (p.e. Norte, sur, este, oeste,.... )\n",
    " - Ordinales: Si que existe una relación de orden entre los valores (Niveles de educación)\n",
    "     - Conservar el labeling numerico para las categoricas ordinales (p.e. A1, A2, B1, B2, C1, .... )\n",
    "\n",
    "A la hora de tratar las nominales, podeis tratarlas como multiples variables binarias donde cada nuevo atributo se corresponde a uno de los valores que existian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encode all string attributes. This assumes each sample contains only one posible value \n",
    "\n",
    "def replace_categorical(df):  \n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            print(f\"{col} is categorical\")\n",
    "            df = pd.concat([df, pd.get_dummies(df[col],prefix=col)], axis=1)\n",
    "            df = df.drop(columns=col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df_x = replace_categorical(train_df_x)\n",
    "test_df_x = replace_categorical(test_df_x)\n",
    "\n",
    "train_df_x = train_df_x.drop(columns=['sex_female', 'smoker_no'])\n",
    "test_df_x = test_df_x.drop(columns=['sex_female', 'smoker_no'])\n",
    "pd.get_dummies(pd.Series(list('acba')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestionar nans\n",
    "La idea tras esto es que minimiceis los datos que vais a perder cuanto teneis muestras incompletas o con errores.\n",
    "Dependiendo de la situación en la que esteis y vuestros datos debereis decantaros por una opcion o por otra.\n",
    "\n",
    "- Si los nans estan concetrados en unas pocas muestras (-5%), podeis simplemente eliminar las muestras.\n",
    "- Si os faltan muchos datos de un mismo atributo (+40%), podeis eliminar el atributo ya que os implica eliminar muchas muestras.    \n",
    "- Podeis rellenar los nans con valores generados a partir de la distribucion de datos que teneis. Ya sea directamente mirando los valores cercanos (media, moda, mediana), o con algormitmos que tengan en cuenta mas de un atributo (usar KNN para asignar la media de k-valores mas cercanos a los datos faltantes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nans(df, strategy='zeros'):\n",
    "    columns = df.columns\n",
    "    for col in columns:    \n",
    "        nans = df[col].isnull().sum()\n",
    "        if nans > 0:\n",
    "            if strategy == 'drop':\n",
    "                df.drop(columns=col, inplace=True)\n",
    "            elif strategy == 'zeros':\n",
    "                df.loc[df[col].isnull(), col] = 0\n",
    "            elif strategy == 'mean':\n",
    "                df.loc[df[col].isnull(), col] = df.loc[~df[col].isnull(). col].mean()\n",
    "            elif strategy == 'mean_noise':\n",
    "                pass\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df_x = handle_nans(train_df_x)\n",
    "test_df_x = handle_nans(test_df_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar el tipo de distribución\n",
    "\n",
    "Podeis hacerlo graficamente mediante histogramas, boxplots, QQPlots, ...\n",
    "\n",
    "O podeis usar distintos test estadisticos para confirmar el tipo de distribucion de cada atributo\n",
    "\n",
    "Desde numpy podeis generar arrays de N elementos que sigan una distribucion especifica para facilitar la comparación con vuestros datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "samples = 2000\n",
    "\n",
    "fig, axes = plt.subplots(5,2, figsize=(10, 20))\n",
    "exp = np.random.exponential(1, samples)\n",
    "norm = np.random.randn(samples,)\n",
    "uni = np.random.uniform(-5, 5, samples)\n",
    "logn = np.random.lognormal(1, .5, samples)\n",
    "bim = np.concatenate([np.random.normal(0, 1, size=samples),np.random.normal(2.5, .5, size=samples // 2)])\n",
    "\n",
    "sns.histplot(x=exp, color='yellow', ax=axes[0,0], bins=30)\n",
    "sns.histplot(x=norm, color='blue', ax=axes[1, 0], bins=30)\n",
    "sns.histplot(x=uni, color='green', ax=axes[2, 0], bins=30)\n",
    "sns.histplot(x=logn, color='red', ax=axes[3, 0], bins=30)\n",
    "sns.histplot(x=bim, color='purple', ax=axes[4, 0], bins=30)\n",
    "\n",
    "sns.boxplot(data=exp, color='yellow', ax=axes[0,1])\n",
    "sns.boxplot(data=norm, color='blue', ax=axes[1, 1])\n",
    "sns.boxplot(data=uni, color='green', ax=axes[2, 1])\n",
    "sns.boxplot(data=logn, color='red', ax=axes[3, 1])\n",
    "sns.boxplot(data=bim, color='purple', ax=axes[4, 1])\n",
    "\n",
    "\n",
    "axes[0,0].set_title('Exponential')\n",
    "axes[1,0].set_title('Normal')\n",
    "axes[2,0].set_title('uniforme')\n",
    "axes[3,0].set_title('Logaritmica')\n",
    "axes[4,0].set_title('Bimodal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dist in enumerate([norm, exp, uni, logn, bim]):\n",
    "\n",
    "    stat, p = normaltest(dist)\n",
    "    alpha = .05\n",
    "\n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        print(f\"The null hypothesis can be rejected\")\n",
    "    else:\n",
    "        print(f\"The null hypothesis cannot be rejected (comes from a normal dist)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacion y Desnormalizacion\n",
    "\n",
    "Cuando trabajeis con datos normalizados, acordaos de desnormalizarlos antes de contextualizar los resultados en vuestro dataset.\n",
    "\n",
    "Tambien teneis que tener en cuenta que la media y la std se han de calcular solo sobre el conjunto de training, ya que si lo hacemos sobre todos la base de datos, estariamos filtrando informacion del conjunto de validacion a nuestor modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar\n",
    "\n",
    "def standarize(x, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        mean = x.mean(0)\n",
    "    if std is None:\n",
    "        std = x.std(0)\n",
    "    \n",
    "    return (x - mean[None, :]) / std[None, :], mean, std\n",
    "\n",
    "train_df_x_norm, mean, std = standarize(train_df_x.values)\n",
    "test_df_x_norm, _, _ = standarize(test_df_x.values, mean, std)\n",
    "\n",
    "train_df_y_norm, mean, std = standarize(train_df_y.values[:, None])\n",
    "test_df_y_norm, _, _ = standarize(test_df_y.values[:, None], mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Amb quin atribut s'assoleix un MSE menor?\n",
    "\n",
    "4. Com influeix la normalització en la regressió?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = pd.concat([train_df_x, train_df_y], axis=1)\n",
    "f, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(correlation_df.corr(), annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()    \n",
    "\n",
    "features = train_df_x.columns\n",
    "train_y = train_df_y_norm\n",
    "test_y = test_df_y_norm\n",
    "\n",
    "mses, r2s = [], []\n",
    "features = features.tolist()\n",
    "\n",
    "mses, r2s = [], []\n",
    "for idx, feature in enumerate(features):\n",
    "    linear_model.fit(train_df_x_norm[:, idx, None], train_df_y_norm)\n",
    "    preds = linear_model.predict(test_df_x_norm[:, idx, None])\n",
    "       \n",
    "    mse = mean_squared_error(test_df_y_norm, preds)\n",
    "    mae = mean_absolute_error(test_df_y_norm, preds)\n",
    "    mses.append(mse)\n",
    "    \n",
    "    r2 = r2_score(test_y, preds)\n",
    "    r2s.append(r2)\n",
    "\n",
    "    print(f\"{feature} - MSE: {mse:.3f}; R2: {r2:.3f}; MAE: {mae:.3f}\")\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "plt.figure(figsize=(20,7)) \n",
    "plt.scatter(features, mses, label='mse')\n",
    "plt.scatter(features, r2s, label='r2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "smoke = linear_model.fit(train_df_x_norm[:, 3, None], train_df_y_norm)\n",
    "preds_smoke = smoke.predict(test_df_x_norm[:, 3, None])\n",
    "age = linear_model.fit(train_df_x_norm[:, 0, None], train_df_y_norm)\n",
    "preds_age = smoke.predict(test_df_x_norm[:, 0, None])\n",
    "\n",
    "\n",
    "sns.lineplot(y=preds_smoke[:,0], x=test_df_x_norm[:, 3], ax=ax[0])\n",
    "sns.scatterplot(y=test_df_y_norm[:,0], x=test_df_x_norm[:, 3], ax=ax[0])\n",
    "\n",
    "\n",
    "sns.lineplot(y=preds_age[:,0], x=test_df_x_norm[:, 0], ax=ax[1])\n",
    "sns.scatterplot(y=test_df_y_norm[:,0], x=test_df_x_norm[:, 0], ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "ax = f.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "linear_model = LinearRegression()    \n",
    "\n",
    "idxs = [4,0]\n",
    "model = linear_model.fit(train_df_x_norm[:, idxs], train_df_y_norm)\n",
    "score = model.score(test_df_x_norm[:, idxs], test_df_y_norm)\n",
    "\n",
    "# plot de los puntos en los 3 ejes: los 2 atributos y el valor real a predecir (y)\n",
    "ax.scatter(test_df_x_norm[:, idxs[0]], test_df_x_norm[:, idxs[1]], test_df_y_norm[:,0])\n",
    "\n",
    "# creamos una malla de samples sobre las que queremos predecir el valor\n",
    "x_pred, y_pred = np.meshgrid(\n",
    "    np.linspace(test_df_x_norm[:, idxs[0]].min(), test_df_x_norm[:, idxs[0]].max(), 100),\n",
    "    np.linspace(test_df_x_norm[:, idxs[1]].min(), test_df_x_norm[:, idxs[1]].max(), 100),\n",
    ")\n",
    "to_pred = np.array([x_pred.flatten(), y_pred.flatten()]).T\n",
    "\n",
    "preds_smoke_age = model.predict(to_pred)\n",
    "\n",
    "# ploteamos todos los puntos que conforman la malla para generar el plano de puntos\n",
    "ax.scatter(x_pred.flatten(), y_pred.flatten(), preds_smoke_age)\n",
    "\n",
    "ax.set_xlabel('idx1')\n",
    "ax.set_ylabel('idx2')\n",
    "ax.set_zlabel('label')\n",
    "\n",
    "print(score, \"by taking into account smoking and age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses, r2s = [], []\n",
    "for i in range(1, 9):\n",
    "    pca = PCA(n_components=i)\n",
    "    x_train = pca.fit_transform(train_df_x_norm)\n",
    "    x_test = pca.transform(test_df_x_norm)\n",
    "\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(x_train, train_df_y_norm)\n",
    "    preds = linear_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(test_df_y_norm, preds)\n",
    "    r2 = r2_score(test_df_y_norm, preds)\n",
    "    print(f\"PCA_{i} - MSE: {mse:.3f}; R2: {r2:.3f}\")\n",
    "    \n",
    "    mses.append(mse)\n",
    "    r2s.append(r2)\n",
    "    \n",
    "plt.plot(mses, label='mse')\n",
    "plt.plot(r2s, label='r2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
